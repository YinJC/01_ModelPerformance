{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pyro.infer.util import torch_item\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal as Normal_torch\n",
    "\n",
    "# python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# pyro\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical, MultivariateNormal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, SGD\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "resize = 32\n",
    "epoch = 30\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0005\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True, transform=transform_train),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transform_test),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 160):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 120):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 60):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, inputs=1):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, stride=1, bias=False)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120, bias=False)\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.fc3 = nn.Linear(84, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(F.softplus(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(F.softplus(self.conv2(out)), 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.softplus(self.fc1(out))\n",
    "        out = F.softplus(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bayesian, self).__init__()\n",
    "        self.net = LeNet(10, 1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def normal_prior(self,name, params):\n",
    "        mu_param = pyro.param('{}_mu'.format(name), torch.randn_like(params))\n",
    "        sigma_param = F.softplus(pyro.param('{}_sigma'.format(name), torch.randn_like(params)))\n",
    "        prior = Normal(loc=mu_param, scale=sigma_param)\n",
    "        return prior\n",
    "    \n",
    "    def mean_field_norm_prior(self, name, params, eps=10e-7):\n",
    "        loc_init = pyro.param('{}_mu'.format(name), torch.normal(mean=torch.zeros_like(params), std=torch.mul(torch.ones_like(params), 0.1)))\n",
    "        untransformed_scale_init = pyro.param('{}_sigma'.format(name), torch.normal(mean=torch.ones_like(params)*(-3), std=torch.mul(torch.ones_like(params), 0.1)))\n",
    "        sigma = eps + F.softplus(untransformed_scale_init)\n",
    "        dist = Normal(loc=loc_init, scale=sigma)\n",
    "        return dist\n",
    "\n",
    "    def fixed_normal_prior(self, params):\n",
    "        dist = Normal(loc=torch.zeros_like(params), scale=torch.ones_like(params))\n",
    "        return dist\n",
    "    \n",
    "    def model(self, x, y):\n",
    "        conv1w_prior = self.fixed_normal_prior(self.net.conv1.weight)\n",
    "        conv2w_prior = self.fixed_normal_prior(self.net.conv2.weight)\n",
    "        fc1w_prior = self.fixed_normal_prior(self.net.fc1.weight)\n",
    "        fc2w_prior = self.fixed_normal_prior(self.net.fc2.weight)\n",
    "        fc3w_prior = self.fixed_normal_prior(self.net.fc3.weight)\n",
    "        \n",
    "        priors = {\n",
    "            'conv1.weight':conv1w_prior,\n",
    "            'conv2.weight':conv2w_prior, \n",
    "            'fc1.weight': fc1w_prior,\n",
    "            'fc2.weight':fc2w_prior,\n",
    "            'fc3.weight':fc3w_prior\n",
    "        }\n",
    "        \n",
    "        # lift module parameters to random variables sampled from the priors\n",
    "        lifted_module = pyro.random_module(\"module\", self.net, priors)\n",
    "        \n",
    "        # sample a classifier\n",
    "        lifted_reg_model = lifted_module()\n",
    "        \n",
    "        p_hat = self.log_softmax(lifted_reg_model(x))\n",
    "        \n",
    "        with pyro.plate('observe_data'):\n",
    "            pyro.sample(\"obs\", Categorical(logits=p_hat), obs=y)\n",
    "    \n",
    "    def guide(self, x, y):\n",
    "        conv1w_prior = self.mean_field_norm_prior('conv1w',self.net.conv1.weight)\n",
    "        conv2w_prior = self.mean_field_norm_prior('conv2w',self.net.conv2.weight)\n",
    "        fc1w_prior = self.mean_field_norm_prior('fc1w',self.net.fc1.weight)\n",
    "        fc2w_prior = self.mean_field_norm_prior('fc2w', self.net.fc2.weight)\n",
    "        fc3w_prior = self.mean_field_norm_prior('fc3w',self.net.fc3.weight)\n",
    "        \n",
    "        priors = {\n",
    "            'conv1.weight':conv1w_prior,\n",
    "            'conv2.weight':conv2w_prior, \n",
    "            'fc1.weight': fc1w_prior,\n",
    "            'fc2.weight':fc2w_prior,\n",
    "            'fc3.weight':fc3w_prior\n",
    "        }\n",
    "        lifted_module = pyro.random_module(\"module\", self.net, priors)\n",
    "        return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Bayesian()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_elbo_kl_annealing(model, guide, *args, **kwargs):\n",
    "    # get the annealing factor and latents to anneal from the keyword\n",
    "    # arguments passed to the model and guide\n",
    "    annealing_factor = kwargs.pop('annealing_factor', 1.0)\n",
    "    # run the guide and replay the model against the guide\n",
    "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n",
    "\n",
    "    elbo = 0.0\n",
    "    # loop through all the sample sites in the model and guide trace and\n",
    "    # construct the loss; note that we scale all the log probabilities of\n",
    "    # samples sites in `latents_to_anneal` by the factor `annealing_factor`\n",
    "    for name, site in model_trace.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            factor = annealing_factor if site[\"name\"].split('$$$')[0] in ['module'] else 1.0\n",
    "            elbo = elbo + factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    for name, site in guide_trace.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            factor = annealing_factor if site[\"name\"].split('$$$')[0] in ['module'] else 1.0\n",
    "            elbo = elbo - factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    return -elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(net.model, net.guide, optim, loss=simple_elbo_kl_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, net):\n",
    "    sampled_models = net.guide(None, None)\n",
    "    yhats = sampled_models(x).data\n",
    "    return yhats\n",
    "\n",
    "def train(e, svi, loader):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    m = math.ceil(len(loader.dataset)/batch_size)\n",
    "    svi.optim = Adam({\"lr\": learning_rate(lr, e), 'weight_decay': weight_decay})\n",
    "    \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        inputs_value = data[0]\n",
    "        targets = data[1]\n",
    "        \n",
    "        x = inputs_value.view(-1, 1, resize, resize).repeat(num_samples, 1, 1, 1).cuda()\n",
    "        y = targets.repeat(num_samples).cuda()\n",
    "        \n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        \n",
    "        x, y = Variable(x), Variable(y)\n",
    "        \n",
    "        loss =svi.step(x, y, annealing_factor=beta)\n",
    "        train_loss += loss\n",
    "        \n",
    "        predicted = torch.argmax(predict(x, svi), dim=1)\n",
    "        correct += predicted.eq(y.data).cpu().sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "#         print('|Epoch:{}/{}|Iter:{}/{}|Loss:{}|Acc:{}'.format(\n",
    "#             e, epoch, batch_idx+1, (len(loader.dataset.train_data)//batch_size)+1, loss, (100*correct/total)/num_samples))\n",
    "    print('================>Epoch: ',e, 'Loss: ', train_loss/(len(loader.dataset.train_data)*num_samples), 'Acc: ', (100*correct/total)/num_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(epoch):\n",
    "    train(e, svi, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './save_model'\n",
    "pyro.get_param_store().save(os.path.join(model_path, 'lenet_bayesian_model_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    yhats = torch.stack(yhats, dim=1)\n",
    "    mean = torch.mean(yhats, 1)\n",
    "    return np.argmax(mean.cpu().numpy(), axis=1)\n",
    "\n",
    "def evaluate(T, loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == np.array(labels)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evaluate(10, test_loader, net)\n",
    "print('T: ', 10, 'Acc: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy remove samples with all probability less than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    yhats = F.softmax(torch.stack(yhats, dim=1), dim=2)\n",
    "    mean = torch.mean(yhats, 1)\n",
    "    return np.argmax(mean.cpu().numpy(), axis=1), mean.cpu().numpy()\n",
    "\n",
    "def evaluate(T, loader, net, threshold=0.2):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_cnt = 0\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted, mean_prob = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        confidence = np.max(mean_prob, axis=1)\n",
    "        idx = [idx for idx in range(confidence.shape[0]) if confidence[idx]>threshold]\n",
    "        all_cnt += len(labels)\n",
    "        total += len(idx)\n",
    "        correct += (predicted[idx] == np.array(labels)[idx]).sum().item()\n",
    "    return (100 * correct / total), all_cnt-total, total/all_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio = evaluate(10, test_loader, net, threshold=0.5)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio = evaluate(20, test_loader, net,threshold=0.5)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio = evaluate(10, test_loader, net,threshold=0.6)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc, skip, ratio = evaluate(20, test_loader, net,threshold=0.6)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    yhats = F.softmax(torch.stack(yhats, dim=1), dim=2)\n",
    "    mean = torch.mean(yhats, 1).cpu().numpy()\n",
    "    \n",
    "    # uncertainty\n",
    "    # yhats [batch * 10 * 10]\n",
    "    p_hat = yhats.cpu().numpy()\n",
    "    aleatoric = np.mean(p_hat*(1-p_hat), axis=1) # batch * 10\n",
    "    epistemic = np.mean(p_hat**2, axis=1) - np.mean(p_hat, axis=1)**2 # batch * 10\n",
    "    return np.argmax(mean, axis=1), mean, aleatoric, epistemic\n",
    "\n",
    "def evaluate(T, loader, net,threshold=0.2):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_cnt = 0\n",
    "    total_alea_thresh = 0\n",
    "    total_epis_thresh = 0\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted, mean_prob, aleatoric, epistemic = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        confidence = np.max(mean_prob, axis=1)\n",
    "        idx = [idx for idx in range(confidence.shape[0]) if confidence[idx]>threshold]\n",
    "        all_cnt += len(labels)\n",
    "        total += len(idx)\n",
    "        correct += (predicted[idx] == np.array(labels)[idx]).sum().item()\n",
    "        \n",
    "        # uncertainty for the best choice\n",
    "        total_alea_thresh += np.choose(predicted, aleatoric.T)[idx].sum().item()\n",
    "        total_epis_thresh += np.choose(predicted, epistemic.T)[idx].sum().item()\n",
    "    return (100 * correct / total), all_cnt-total, total/all_cnt, total_alea_thresh/total, total_epis_thresh/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio, mean_alea, mean_epis = evaluate(10, test_loader, net,threshold=0.5)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)\n",
    "print('mean epistemic:', mean_epis)\n",
    "print('mean aleaotoric:', mean_alea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio, mean_alea, mean_epis = evaluate(20, test_loader, net,threshold=0.5)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)\n",
    "print('mean epistemic:', mean_epis)\n",
    "print('mean aleaotoric:', mean_alea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio, mean_alea, mean_epis = evaluate(10, test_loader, net,threshold=0.6)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)\n",
    "print('mean epistemic:', mean_epis)\n",
    "print('mean aleaotoric:', mean_alea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, skip, ratio, mean_alea, mean_epis = evaluate(20, test_loader, net,threshold=0.6)\n",
    "print('accuracy is: ', acc)\n",
    "print('number of samples skipped :', skip)\n",
    "print('raio (able to predict/all sample):', ratio)\n",
    "print('mean epistemic:', mean_epis)\n",
    "print('mean aleaotoric:', mean_alea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyse the sample with confidence over 0.6 but prediction is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    yhats = F.softmax(torch.stack(yhats, dim=1), dim=2)\n",
    "    mean = torch.mean(yhats, 1).cpu().numpy()\n",
    "    \n",
    "    # uncertainty\n",
    "    # yhats [batch * 10 * 10]\n",
    "    p_hat = yhats.cpu().numpy()\n",
    "    aleatoric = np.mean(p_hat*(1-p_hat), axis=1) # batch * 10\n",
    "    epistemic = np.mean(p_hat**2, axis=1) - np.mean(p_hat, axis=1)**2 # batch * 10\n",
    "    return np.argmax(mean, axis=1), mean, aleatoric, epistemic\n",
    "\n",
    "def evaluate(T, loader, net, threshold=0.2):\n",
    "    cnt = 0\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted, mean_prob, aleatoric, epistemic = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        confidence = np.max(mean_prob, axis=1)\n",
    "        idx = [idx for idx in range(confidence.shape[0]) if confidence[idx]>threshold]\n",
    "        correct = (predicted[idx] == np.array(labels)[idx])\n",
    "        wrong_idx = [i for i in range(len(correct)) if correct[i] == False]\n",
    "        usable_idx = np.array(idx)[wrong_idx]\n",
    "        if len(wrong_idx)>0:\n",
    "            cnt += 1\n",
    "            if cnt>1:\n",
    "                return images[usable_idx], labels[usable_idx], mean_prob[usable_idx], aleatoric[usable_idx], epistemic[usable_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, prob, alea, epis = evaluate(10, test_loader, net, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "plt.figure()\n",
    "plt.imshow(image[num].numpy().squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('confidence:')\n",
    "print(prob[num])\n",
    "print('label:')\n",
    "print(label[num].numpy())\n",
    "print('prediction')\n",
    "print(np.argmax(prob[num]))\n",
    "print('alea ')\n",
    "print(alea[num])\n",
    "print('epis')\n",
    "print(epis[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Calibration Error (ECE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, softmaxes, labels):\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        ece = torch.zeros(1)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece = ECELoss(n_bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [F.softmax(model(x).data, dim=1) for model in sampled_models]\n",
    "    yhats = torch.stack(yhats, dim=1)\n",
    "    mean = torch.mean(yhats, 1)\n",
    "    return mean\n",
    "\n",
    "def evaluate(T, loader, net):\n",
    "    prob_list = []\n",
    "    label_list = []\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        label_list.extend(labels)\n",
    "        prob_list.append(predicted)\n",
    "    label_list = torch.stack(label_list, dim=0).view(-1).cpu()\n",
    "    prob_list = torch.cat(prob_list, dim=0).cpu()  \n",
    "    return ece.forward(prob_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_loss = evaluate(10, test_loader, net)\n",
    "print('ece_loss:', str(ece_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReliabilityDiagram(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins=10):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ReliabilityDiagram, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, softmaxes, labels):\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                x.append(avg_confidence_in_bin)\n",
    "                y.append(accuracy_in_bin)\n",
    "        return torch.stack(x, dim=0).view(-1).cpu().numpy(), torch.stack(y, dim=0).view(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = ReliabilityDiagram(n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [F.softmax(model(x).data, dim=1) for model in sampled_models]\n",
    "    yhats = torch.stack(yhats, dim=1)\n",
    "    mean = torch.mean(yhats, 1)\n",
    "    return mean\n",
    "\n",
    "def evaluate(T, loader, net):\n",
    "    prob_list = []\n",
    "    label_list = []\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        label_list.extend(labels)\n",
    "        prob_list.append(predicted)\n",
    "    label_list = torch.stack(label_list, dim=0).view(-1).cpu()\n",
    "    prob_list = torch.cat(prob_list, dim=0).cpu()  \n",
    "    return label_list, prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, prob = evaluate(10, test_loader, net)\n",
    "x,y = rd(prob, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(x=x, height=y, width=0.05, label='output')\n",
    "plt.plot(np.linspace(0, 1, 11), np.linspace(0, 1, 11), 'r', '--')\n",
    "plt.xlabel('confidence')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Reliability Diagram')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Model Don't Know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('fashion-mnist-data/', train=False, download=True, transform=transform_train\n",
    "                       ),\n",
    "        batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, T, net):\n",
    "    sampled_models = [net.guide(None, None) for _ in range(T)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    yhats = F.softmax(torch.stack(yhats, dim=1), dim=2)\n",
    "    mean = torch.mean(yhats, 1).cpu().numpy()\n",
    "    \n",
    "    # uncertainty\n",
    "    # yhats [batch * 10 * 10]\n",
    "    p_hat = yhats.cpu().numpy()\n",
    "    aleatoric = np.mean(p_hat*(1-p_hat), axis=1) # batch * 10\n",
    "    epistemic = np.mean(p_hat**2, axis=1) - np.mean(p_hat, axis=1)**2 # batch * 10\n",
    "    return np.argmax(mean, axis=1), mean, aleatoric, epistemic\n",
    "\n",
    "def evaluate(T, loader, net,threshold=0.2):\n",
    "    entropy = 0\n",
    "    total = 0\n",
    "    all_cnt = 0\n",
    "    total_alea_thresh = 0\n",
    "    total_epis_thresh = 0\n",
    "    entropy = np.array([])\n",
    "    for j, data in enumerate(loader):\n",
    "        images, labels = data\n",
    "        predicted, mean_prob, aleatoric, epistemic = predict(images.view(-1, 1, 32, 32).cuda(), T, net)\n",
    "        confidence = np.max(mean_prob, axis=1)\n",
    "        idx = [idx for idx in range(confidence.shape[0]) if confidence[idx]>threshold]\n",
    "        all_cnt += len(labels)\n",
    "        total += len(idx)\n",
    "        entropy = np.concatenate([entropy, confidence])\n",
    "        # uncertainty for the best choice\n",
    "        total_alea_thresh += np.choose(predicted, aleatoric.T).sum().item()\n",
    "        total_epis_thresh += np.choose(predicted, epistemic.T).sum().item()\n",
    "    entropy = -np.log(entropy)\n",
    "    return all_cnt-total, total/all_cnt, total_alea_thresh/total, total_epis_thresh/total, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip, ratio, alea_mean, epis_mean, entropy = evaluate(10, test_loader, net,threshold=0.5)\n",
    "print('number of sample skipped ', skip)\n",
    "print('predict ratio ',ratio)\n",
    "print('mean alea ', alea_mean)\n",
    "print('mean epis ', epis_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_cnt = pd.Series(entropy).value_counts().sort_index()\n",
    "cumulative = np.cumsum(entropy_cnt.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(entropy_cnt.index, cumulative/cumulative[-1], 'r', label='cdf')\n",
    "plt.xlabel('entropy')\n",
    "plt.ylabel('cdf')\n",
    "plt.title('Empirical CDF of Entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepr)",
   "language": "python",
   "name": "deepr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
