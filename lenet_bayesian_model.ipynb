{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pyro.infer.util import torch_item\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal as Normal_torch\n",
    "\n",
    "# python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# pyro\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical, MultivariateNormal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, SGD\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "resize = 32\n",
    "epoch = 200\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0005\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True, transform=transform_train),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transform_test),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 160):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 120):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 60):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, inputs=1):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, stride=1, bias=False)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120, bias=False)\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.fc3 = nn.Linear(84, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(F.softplus(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(F.softplus(self.conv2(out)), 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.softplus(self.fc1(out))\n",
    "        out = F.softplus(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bayesian, self).__init__()\n",
    "        self.net = LeNet(10, 1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def normal_prior(self,name, params):\n",
    "        mu_param = pyro.param('{}_mu'.format(name), torch.randn_like(params))\n",
    "        sigma_param = F.softplus(pyro.param('{}_sigma'.format(name), torch.randn_like(params)))\n",
    "        prior = Normal(loc=mu_param, scale=sigma_param)\n",
    "        return prior\n",
    "    \n",
    "    def mean_field_norm_prior(self, name, params, eps=10e-7):\n",
    "        loc_init = pyro.param('{}_mu'.format(name), torch.normal(mean=torch.zeros_like(params), std=torch.mul(torch.ones_like(params), 0.1)))\n",
    "        untransformed_scale_init = pyro.param('{}_sigma'.format(name), torch.normal(mean=torch.ones_like(params)*(-3), std=torch.mul(torch.ones_like(params), 0.1)))\n",
    "        sigma = eps + F.softplus(untransformed_scale_init)\n",
    "        dist = Normal(loc=loc_init, scale=sigma)\n",
    "        return dist\n",
    "\n",
    "    def fixed_normal_prior(self, params):\n",
    "        dist = Normal(loc=torch.zeros_like(params), scale=torch.ones_like(params))\n",
    "        return dist\n",
    "    \n",
    "    def model(self, x, y):\n",
    "        conv1w_prior = self.fixed_normal_prior(self.net.conv1.weight)\n",
    "        conv2w_prior = self.fixed_normal_prior(self.net.conv2.weight)\n",
    "        fc1w_prior = self.fixed_normal_prior(self.net.fc1.weight)\n",
    "        fc2w_prior = self.fixed_normal_prior(self.net.fc2.weight)\n",
    "        fc3w_prior = self.fixed_normal_prior(self.net.fc3.weight)\n",
    "        \n",
    "        priors = {\n",
    "            'conv1.weight':conv1w_prior,\n",
    "            'conv2.weight':conv2w_prior, \n",
    "            'fc1.weight': fc1w_prior,\n",
    "            'fc2.weight':fc2w_prior,\n",
    "            'fc3.weight':fc3w_prior\n",
    "        }\n",
    "        \n",
    "        # lift module parameters to random variables sampled from the priors\n",
    "        lifted_module = pyro.random_module(\"module\", self.net, priors)\n",
    "        \n",
    "        # sample a classifier\n",
    "        lifted_reg_model = lifted_module()\n",
    "        \n",
    "        p_hat = self.log_softmax(lifted_reg_model(x))\n",
    "        \n",
    "        with pyro.plate('observe_data'):\n",
    "            pyro.sample(\"obs\", Categorical(logits=p_hat), obs=y)\n",
    "    \n",
    "    def guide(self, x, y):\n",
    "        conv1w_prior = self.mean_field_norm_prior('conv1w',self.net.conv1.weight)\n",
    "        conv2w_prior = self.mean_field_norm_prior('conv2w',self.net.conv2.weight)\n",
    "        fc1w_prior = self.mean_field_norm_prior('fc1w',self.net.fc1.weight)\n",
    "        fc2w_prior = self.mean_field_norm_prior('fc2w', self.net.fc2.weight)\n",
    "        fc3w_prior = self.mean_field_norm_prior('fc3w',self.net.fc3.weight)\n",
    "        \n",
    "        priors = {\n",
    "            'conv1.weight':conv1w_prior,\n",
    "            'conv2.weight':conv2w_prior, \n",
    "            'fc1.weight': fc1w_prior,\n",
    "            'fc2.weight':fc2w_prior,\n",
    "            'fc3.weight':fc3w_prior\n",
    "        }\n",
    "        lifted_module = pyro.random_module(\"module\", self.net, priors)\n",
    "        return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bayesian(\n",
       "  (net): LeNet(\n",
       "    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "    (fc1): Linear(in_features=400, out_features=120, bias=False)\n",
       "    (fc2): Linear(in_features=120, out_features=84, bias=False)\n",
       "    (fc3): Linear(in_features=84, out_features=10, bias=False)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Bayesian()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_elbo_kl_annealing(model, guide, *args, **kwargs):\n",
    "    # get the annealing factor and latents to anneal from the keyword\n",
    "    # arguments passed to the model and guide\n",
    "    annealing_factor = kwargs.pop('annealing_factor', 1.0)\n",
    "    # run the guide and replay the model against the guide\n",
    "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n",
    "\n",
    "    elbo = 0.0\n",
    "    # loop through all the sample sites in the model and guide trace and\n",
    "    # construct the loss; note that we scale all the log probabilities of\n",
    "    # samples sites in `latents_to_anneal` by the factor `annealing_factor`\n",
    "    for name, site in model_trace.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            factor = annealing_factor if site[\"name\"].split('$$$')[0] in ['module'] else 1.0\n",
    "            elbo = elbo + factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    for name, site in guide_trace.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            factor = annealing_factor if site[\"name\"].split('$$$')[0] in ['module'] else 1.0\n",
    "            elbo = elbo - factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
    "    return -elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(net.model, net.guide, optim, loss=simple_elbo_kl_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, net):\n",
    "    sampled_models = net.guide(None, None)\n",
    "    yhats = sampled_models(x).data\n",
    "    return yhats\n",
    "\n",
    "def train(e, svi, loader):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    m = math.ceil(len(loader.dataset)/batch_size)\n",
    "    svi.optim = Adam({\"lr\": learning_rate(lr, e), 'weight_decay': weight_decay})\n",
    "    \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        inputs_value = data[0]\n",
    "        targets = data[1]\n",
    "        \n",
    "        x = inputs_value.view(-1, 1, resize, resize).repeat(num_samples, 1, 1, 1).cuda()\n",
    "        y = targets.repeat(num_samples).cuda()\n",
    "        \n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        \n",
    "        x, y = Variable(x), Variable(y)\n",
    "        \n",
    "        loss =svi.step(x, y, annealing_factor=beta)\n",
    "        train_loss += loss\n",
    "        \n",
    "        predicted = torch.argmax(predict(x, svi), dim=1)\n",
    "        correct += predicted.eq(y.data).cpu().sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "#         print('|Epoch:{}/{}|Iter:{}/{}|Loss:{}|Acc:{}'.format(\n",
    "#             e, epoch, batch_idx+1, (len(loader.dataset.train_data)//batch_size)+1, loss, (100*correct/total)/num_samples))\n",
    "    print('================>Epoch: ',e, 'Loss: ', train_loss/(len(loader.dataset.train_data)*num_samples), 'Acc: ', (100*correct/total)/num_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================>Epoch:  0 Loss:  1.7845463197199503 Acc:  46.156666666666666\n",
      "================>Epoch:  1 Loss:  0.7744766996860504 Acc:  83.71166666666667\n",
      "================>Epoch:  2 Loss:  0.5725195897332828 Acc:  90.28333333333333\n",
      "================>Epoch:  3 Loss:  0.4897542996160189 Acc:  92.995\n",
      "================>Epoch:  4 Loss:  0.44091435690402986 Acc:  94.47999999999999\n",
      "================>Epoch:  5 Loss:  0.4104387205455204 Acc:  95.35666666666667\n",
      "================>Epoch:  6 Loss:  0.39237090891649323 Acc:  95.85666666666667\n",
      "================>Epoch:  7 Loss:  0.3762465294449528 Acc:  96.35333333333332\n",
      "================>Epoch:  8 Loss:  0.36157213926270604 Acc:  96.80833333333334\n",
      "================>Epoch:  9 Loss:  0.3526995850767195 Acc:  97.02166666666668\n",
      "================>Epoch:  10 Loss:  0.34565842766799654 Acc:  97.35499999999999\n",
      "================>Epoch:  11 Loss:  0.3378770655168717 Acc:  97.46833333333333\n",
      "================>Epoch:  12 Loss:  0.3312405697915765 Acc:  97.68166666666667\n",
      "================>Epoch:  13 Loss:  0.32586036839966975 Acc:  97.85166666666666\n",
      "================>Epoch:  14 Loss:  0.32407087459364287 Acc:  97.88666666666667\n",
      "================>Epoch:  15 Loss:  0.31771925147068064 Acc:  98.00333333333333\n",
      "================>Epoch:  16 Loss:  0.31537068799411877 Acc:  98.09333333333333\n",
      "================>Epoch:  17 Loss:  0.3128895866244426 Acc:  98.10833333333333\n",
      "================>Epoch:  18 Loss:  0.30891502723727843 Acc:  98.30333333333333\n",
      "================>Epoch:  19 Loss:  0.3045616809982248 Acc:  98.325\n",
      "================>Epoch:  20 Loss:  0.30285256834288127 Acc:  98.36\n",
      "================>Epoch:  21 Loss:  0.29959197425303746 Acc:  98.39166666666667\n",
      "================>Epoch:  22 Loss:  0.2981082414656381 Acc:  98.42333333333333\n",
      "================>Epoch:  23 Loss:  0.29655205982877875 Acc:  98.52000000000001\n",
      "================>Epoch:  24 Loss:  0.2919625716624254 Acc:  98.54166666666666\n",
      "================>Epoch:  25 Loss:  0.2896741925232764 Acc:  98.52666666666667\n",
      "================>Epoch:  26 Loss:  0.28843758270400266 Acc:  98.65833333333333\n",
      "================>Epoch:  27 Loss:  0.2867877344762588 Acc:  98.66499999999999\n",
      "================>Epoch:  28 Loss:  0.28456508093664423 Acc:  98.72166666666666\n",
      "================>Epoch:  29 Loss:  0.28385582572832235 Acc:  98.71166666666667\n",
      "================>Epoch:  30 Loss:  0.28094945033791513 Acc:  98.775\n",
      "================>Epoch:  31 Loss:  0.2786532143428642 Acc:  98.78\n",
      "================>Epoch:  32 Loss:  0.2783168745021879 Acc:  98.83500000000001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fb7ecfa52ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-bacce7863303>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, svi, loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannealing_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepr/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda3/envs/deepr/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36m_loss_and_grads\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_and_grads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0m_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                     \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-360a8515d36c>\u001b[0m in \u001b[0;36msimple_elbo_kl_annealing\u001b[0;34m(model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannealing_factor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$$$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'module'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melbo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    train(e, svi, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepr)",
   "language": "python",
   "name": "deepr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
